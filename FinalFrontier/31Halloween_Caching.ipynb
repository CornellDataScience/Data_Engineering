{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import timeit as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/test.csv', inferSchema=True, header=True)\n",
    "dfbig= spark.read.json('/review.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypotethetically starting to test the speeds of different cachce types. Have more planned but got hung up on the compile process hanging and had to fix that issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first try without cacheing\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this would hypothetically work, if I had the memory available to cahce the data to\n",
    "dfbig.cache()\n",
    "dfbig.groupBy(col(\"stars\")).count().collect() #had an output, but then cleverly restarted the kernell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist to disk as opposed to memory\n",
    "dfbig.persist(DISK_ONLY)\n",
    "dfbig.groupBy(col(\"stars\")).count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Persist vs. cache](https://stackoverflow.com/questions/26870537/what-is-the-difference-between-cache-and-persist)\n",
    "- Persist: can store in other areas (ie. disk) \n",
    "    - On-heap: subject to garbage collection\n",
    "    - Off-heap: serialized data not subject to garbage collection\n",
    "        Slightly slower than memory-based (have to be deserialized still)\n",
    "        Still faster than disk-based\n",
    "        Don’t waste GC’s time\n",
    "        Java and Scala only!\n",
    "- Cache: store in memory only\n",
    "- Storage Levels: \n",
    "    - DISK_ONLY\n",
    "    - MEMORY_ONLY\n",
    "    - MEMORY_AND_DISK\n",
    "    - Appending a number (ie. DISK_ONLY_2) adds that number of replicas\n",
    "    - Appending SER (ie MEMORY_ONLY_SER) will serialize the data --> only in Java/Scala\n",
    "\n",
    "# [RDDs](https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd)\t\n",
    "- Java/Scala only\n",
    "- Nothing happens to data until actually perform an operation on it\n",
    "    - Only references\n",
    "- Linear situation: ie load file into rdd, perform a basic transformation, then count\n",
    "    - Cache not needed\n",
    "    - Data loaded to executors, transform, and count computed all in memory\n",
    "- Non-linear: ie. load file, want to create a filtered dataset to work with\n",
    "    - Cache before begin to branch\n",
    "- Rule of thumb: cache when RDD branching out, used multiple times in a loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Serializers\n",
    "- Best practice to cache serialized objects\n",
    "- Pickle Serializer \n",
    "    - More universal\n",
    "- Marshall Serializer\n",
    "    - Faster\n",
    "- [TBD whether either of these actually offer an advantage over Kyros](https://stackoverflow.com/questions/36278574/do-you-benefit-from-the-kryo-serializer-when-you-use-pyspark)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
